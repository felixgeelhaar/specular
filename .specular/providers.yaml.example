# AI Provider Configuration
# This file configures AI providers that specular can use for various tasks

providers:
  # Local Ollama provider (requires ollama to be installed)
  - name: ollama
    type: cli
    enabled: true
    source: local
    version: "1.0.0"
    config:
      # Path to the ollama provider wrapper executable
      path: ./providers/ollama/ollama-provider
      # Trust level: builtin, verified, or community
      trust_level: community
      # Provider capabilities
      capabilities:
        streaming: true
        tools: false
        multi_turn: true
        max_context_tokens: 8192
    # Model hints mapping
    models:
      fast: llama3.2
      codegen: codellama
      cheap: llama3.2
      agentic: llama3.2

  # OpenAI API provider (requires OPENAI_API_KEY)
  - name: openai
    type: api
    enabled: false
    source: builtin
    version: "1.0.0"
    config:
      api_key: ${OPENAI_API_KEY}
      base_url: https://api.openai.com/v1
      capabilities:
        streaming: true
        tools: true
        multi_turn: true
        max_context_tokens: 128000
    models:
      fast: gpt-4o-mini
      codegen: gpt-4o
      long-context: gpt-4-turbo
      cheap: gpt-4o-mini
      agentic: gpt-4o

  # Anthropic Claude API provider (requires ANTHROPIC_API_KEY)
  - name: anthropic
    type: api
    enabled: false
    source: builtin
    version: "1.0.0"
    config:
      api_key: ${ANTHROPIC_API_KEY}
      base_url: https://api.anthropic.com/v1
      capabilities:
        streaming: true
        tools: true
        multi_turn: true
        max_context_tokens: 200000
    models:
      fast: claude-haiku-3.5
      codegen: claude-sonnet-3.5
      agentic: claude-sonnet-4
      long-context: claude-sonnet-3.5

  # Claude CLI provider (requires claude CLI to be installed)
  - name: claude-cli
    type: cli
    enabled: false
    source: local
    version: "1.0.0"
    config:
      path: /opt/homebrew/bin/claude
      args: ["--print", "--output-format", "json"]
      trust_level: verified
      capabilities:
        streaming: false
        tools: true
        multi_turn: false
        max_context_tokens: 200000
    models:
      fast: haiku
      codegen: sonnet
      agentic: opus

# Provider selection strategy
strategy:
  # Prefer providers in this order when multiple are available
  preference:
    - ollama      # Local first (fastest, free)
    - claude-cli  # Local Claude CLI (free)
    - anthropic   # Cloud API (high quality)
    - openai      # Cloud API (fallback)

  # Budget constraints
  budget:
    max_cost_per_day: 20.0  # USD
    max_cost_per_request: 1.0  # USD

  # Performance requirements
  performance:
    max_latency_ms: 60000  # 60 seconds
    prefer_cheap: true      # Prefer cheaper models when quality is similar

  # Fallback behavior
  fallback:
    enabled: true
    max_retries: 3
    retry_delay_ms: 1000
    fallback_model: ollama/llama3.2
